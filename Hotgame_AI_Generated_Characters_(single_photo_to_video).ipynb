{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunhaha123/-/blob/master/Hotgame_AI_Generated_Characters_(single_photo_to_video).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKvX_88sNgXs"
      },
      "source": [
        "# AI Generated Characters for Learning and Wellbeing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO7671Y9oXiW"
      },
      "source": [
        "Website: https://www.media.mit.edu/projects/ai-generated-characters/overview/\n",
        "\n",
        "Paper: https://www.nature.com/articles/s42256-021-00417-9\n",
        "\n",
        "Github: https://github.com/mitmedialab/AI-generated-characters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=17arRYqt6QyEjkj4-5eDrqRPcteTsbheO)\n"
      ],
      "metadata": {
        "id": "9M320pz78nl7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XSW0Yc6Nq-F"
      },
      "source": [
        "*This notebook is a combination of previous work on AI generated characters compiled into one easy to use pipeline that include [Siarohin et al.](https://github.com/AliaksandrSiarohin/first-order-model), [Prajwal et al.](https://github.com/Rudrabha/Wav2Lip), and [Corentin](https://github.com/CorentinJ/Real-Time-Voice-Cloning). Please go check out their amazing work.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbvav0P_NNqj"
      },
      "source": [
        "**Licensed under the MIT License**\n",
        "\n",
        "\n",
        "Copyright (c) 2021 MIT Media Lab\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "The above copyright notice and this permission notice shall be included in\n",
        "all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "THE SOFTWARE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t-npGOwrGNhs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "132d7e5d-614f-4808-fe0a-e0d46849a496"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Downloading Packages\n",
            "Cloning into 'Wav2Lip'...\n",
            "remote: Enumerating objects: 360, done.\u001b[K\n",
            "remote: Counting objects: 100% (22/22), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 360 (delta 11), reused 15 (delta 6), pack-reused 338\u001b[K\n",
            "Receiving objects: 100% (360/360), 522.87 KiB | 4.84 MiB/s, done.\n",
            "Resolving deltas: 100% (195/195), done.\n",
            "Cloning into 'first-order-model'...\n",
            "remote: Enumerating objects: 299, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 299 (delta 2), reused 2 (delta 0), pack-reused 293\u001b[K\n",
            "Receiving objects: 100% (299/299), 72.15 MiB | 28.43 MiB/s, done.\n",
            "Resolving deltas: 100% (153/153), done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84031"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#markdown #**Installation of libraries**\n",
        "# markdown This cell will take a little while because it has to download several libraries.\n",
        "%cd \"/content\" \n",
        "import requests\n",
        "\n",
        "print(\"Downloading Packages\")\n",
        "# Character Images\n",
        "!gdown --id \"16HzQKA4e3vpLY8Em57WnE8UwIE591aF1\" -O \"/content/mona_lisa.png\" &> /dev/null\n",
        "!gdown --id \"1cgfFgzm4BrqKIkyspGib6u4ty5ReyeM_\" -O \"/content/einstein.png\" &> /dev/null\n",
        "!gdown --id \"10N3e5E0R1aYcLVmE_dmtMCSYVFGQLTeq\" -O \"/content/lincoln.png\" &> /dev/null\n",
        "!gdown --id \"1-BeSNGGjJADs5W-Rn6izAteuVzJcnhW1\" -O \"/content/nietzsche.png\" &> /dev/null\n",
        "!gdown --id \"1zPPUQ7xgbhnpVNl26J1Gl6rXlJ6g0rK7\" -O \"/content/sokrates.png\" &> /dev/null\n",
        "!gdown --id \"1mzzEdXEOohLcpr8L01JzOVbirEMJogni\" -O \"/content/van_gogh.png\" &> /dev/null\n",
        "\n",
        "# Face Cropping\n",
        "!wget \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_alt2.xml\" -O \"/content/haarcascade_frontalface_alt2.xml\" &> /dev/null\n",
        "\n",
        "# Wav2Lip\n",
        "!git clone \"https://github.com/Rudrabha/Wav2Lip.git\"\n",
        "!wget \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" -O \"Wav2Lip/face_detection/detection/sfd/s3fd.pth\" &> /dev/null\n",
        "!gdown --id \"1IKhxXy0mplOpGFWLH9_uUhBoIplao8j0\" -O \"/content/Wav2Lip/checkpoints/wav2lip_gan.pth\" &> /dev/null\n",
        "\n",
        "# First-Order-Model\n",
        "!git clone \"https://github.com/AliaksandrSiarohin/first-order-model\"\n",
        "!gdown --id \"19d9ZJYAMsNNQZd4AzIWCw4sF1EaNYuJ3\" -O \"/content/first-order-model/vox-cpk.pth.tar\" &> /dev/null\n",
        "\n",
        "# Template Data\n",
        "#!gdown --id \"1Qod7I5hiK1nCPsHBqAdK6hoYZgNzQPHi\" -O \"driving_video_long.mp4\"\n",
        "!gdown --id \"1o2zD5xky8F6wZ21PkeG5KhJOlSdkeEpm\" -O \"driving_video.mp4\" &> /dev/null\n",
        "\n",
        "# Watermark\n",
        "url = 'https://raw.githubusercontent.com/mitmedialab/AI-generated-characters/main/gen.png'\n",
        "r = requests.get(url, allow_redirects=True) \n",
        "open('gen.png', 'wb').write(r.content)\n",
        "\n",
        "# Noise\n",
        "url = 'https://raw.githubusercontent.com/mitmedialab/AI-generated-characters/main/noise2.jpg'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "open('noise_2.png', 'wb').write(r.content)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# General Functions\n",
        "print(\"Loading Libraries and functions\")\n",
        "import sys\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from io import StringIO\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display, Audio, clear_output\n",
        "#from dl_colab_notebooks.audio import record_audio, upload_audio\n",
        "from scipy.io import wavfile\n",
        "\n",
        "class IpyExit(SystemExit):\n",
        "    \"\"\"\n",
        "    Exit Exception for IPython.\n",
        "    Exception temporarily redirects stderr to buffer.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        print(\"Error: Please only select one input. If you will not use text please leave text field empty.\")\n",
        "        sys.stderr = StringIO()\n",
        "\n",
        "    def __del__(self):\n",
        "        sys.stderr.close()\n",
        "        sys.stderr = sys.__stderr__  # restore from backup\n",
        "\n",
        "from google.colab import files\n",
        "def getLocalFiles():\n",
        "  uploaded = files.upload()\n",
        "  filename = next(iter(uploaded))\n",
        "  return filename"
      ],
      "metadata": {
        "id": "gY_ZYUZhY0Db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455e0e39-bb55-4240-beb6-aa7778961ecd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Libraries and functions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First-order-model\n",
        "import imageio\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from skimage.transform import resize\n",
        "from IPython.display import HTML\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def _compute_embedding(audio):\n",
        "    display(Audio(audio, rate=SAMPLE_RATE, autoplay=True))\n",
        "    global embedding\n",
        "    embedding = None\n",
        "    embedding = encoder.embed_utterance(encoder.preprocess_wav(audio, SAMPLE_RATE))\n",
        "\n",
        "def _record_audio(b):\n",
        "  clear_output()\n",
        "  audio = record_audio(record_seconds, sample_rate=SAMPLE_RATE)\n",
        "  #_compute_embedding(audio)\n",
        "  display(Audio(audio, rate=SAMPLE_RATE, autoplay=True))\n",
        "  wavfile.write('driving_audio.wav', SAMPLE_RATE, (32767*audio).astype(np.int16))\n",
        "\n",
        "def _upload_audio(b):\n",
        "  clear_output()\n",
        "  audio = upload_audio(sample_rate=SAMPLE_RATE)\n",
        "  _compute_embedding(audio)\n",
        "\n",
        "def trim_img(img_src):\n",
        "  \n",
        "  import imutils\n",
        "\n",
        "  # Read the Input Image\n",
        "  img = cv2.imread(img_src)\n",
        "  img = imutils.resize(img, width=400)  \n",
        "\n",
        "  # Convert into grayscale\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  # Trim to 400x400\n",
        "  face_cascade = cv2.CascadeClassifier('/content/haarcascade_frontalface_alt2.xml')\n",
        "  faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
        "  try:\n",
        "    for (x, y, w, h) in faces:\n",
        "      extention = 40\n",
        "      faces = img[y-extention:y + h+extention, x-extention:x + w + extention]\n",
        "      cv2.imwrite('/content/img_trimmed.png', faces)\n",
        "  except:\n",
        "    print(\"Error: Face takes too much space on image. Try a different image, or trim it yourself to 400x400.\")\n",
        "\n",
        "  return \"/content/img_trimmed.png\"\n",
        "\n",
        "\n",
        "def animate_video(img_filename, vid_filename):\n",
        "    %cd /content/first-order-model/\n",
        "    \n",
        "    from demo import make_animation\n",
        "    from demo import load_checkpoints\n",
        "    from skimage import img_as_ubyte\n",
        "\n",
        "    source_image = imageio.imread(img_filename)\n",
        "    driving_video = imageio.mimread(vid_filename, fps=30, memtest=False) \n",
        "\n",
        "    # Resize image and video to 256x256\n",
        "    source_image = resize(source_image, (256, 256))[..., :3]\n",
        "    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n",
        "\n",
        "    # Load Model\n",
        "    generator, kp_detector = load_checkpoints(config_path='config/vox-256.yaml', checkpoint_path='/content/first-order-model/vox-cpk.pth.tar')\n",
        "\n",
        "    # Make Animation\n",
        "    predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=True,\n",
        "                                adapt_movement_scale=False)\n",
        "    #save resulting video\n",
        "    imageio.mimsave('/content/vidvid.mp4', [img_as_ubyte(frame) for frame in predictions], fps=30)\n",
        "\n",
        "    %cd /content\n",
        "\n",
        "\n",
        "def tracability(video_filename):\n",
        "  import moviepy.editor as mp\n",
        "\n",
        "  video = mp.VideoFileClip(video_filename)\n",
        "\n",
        "  machine = (mp.ImageClip('/content/noise_2.png')\n",
        "    .set_duration(video.duration)\n",
        "    .set_opacity(.05)\n",
        "    .resize(height = 552) #\n",
        "    .margin(right = 0, top = 0, opacity = 1.0)\n",
        "    .set_pos((\"center\", \"center\")))\n",
        "  \n",
        "  human = (mp.ImageClip('/content/gen.png')\n",
        "   .set_duration(video.duration)\n",
        "   .resize(height = 50) #\n",
        "   .margin(right = 0, top = 0, opacity = 1.0)\n",
        "   .set_pos((\"left\", \"bottom\")))\n",
        "\n",
        "  final = mp.CompositeVideoClip([video, machine, human])\n",
        "  final.write_videofile(\"/content/marked.mp4\")\n",
        "\n",
        "print(\"Succesfully Finished Installing Libraries\")"
      ],
      "metadata": {
        "id": "AxMCDiMR0f_h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96543840-d30b-4760-9171-44e2ed14c45f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Succesfully Finished Installing Libraries\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rEvjGeswFb1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531c55c3-87ed-4d65-fd34-bae59a77e6e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mona Lisa selected.\n"
          ]
        }
      ],
      "source": [
        "#@markdown #**Choose Character**\n",
        "\n",
        "# TO DO: Show Images of Characters one can choose.\n",
        "\n",
        "# @markdown Choose the character which you want to animate. If you have any requests for new characters to animate, please let us know here: patpat@mit.edu\n",
        "character = 'Mona Lisa' #@param [\"Van Gogh\", \"Mona Lisa\", \"Einstein\", \"Lincoln\", \"Nietzsche\", \"Sokrates\", \"Upload Your Own\"]\n",
        "print(f\"{character} selected.\")\n",
        "\n",
        "if character == \"Upload Your Own\":\n",
        "  character_img = \"/content/\"+getLocalFiles()\n",
        "  if cv2.imread(character_img).shape[0] != cv2.imread(character_img).shape[1]:\n",
        "    print(\"Cropping uploaded image\")\n",
        "    character_img = trim_img(character_img)\n",
        "\n",
        "else:\n",
        "  character = character.lower().replace(\" \", \"_\") # make lowercase and remove spacing\n",
        "  character_img = \"/content/\"+character+\".png\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "character_img = \"/content/CC5.jpg\" ########################"
      ],
      "metadata": {
        "id": "3rJ4q_ZRs0rQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xGIhE54sFPXG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82,
          "referenced_widgets": [
            "655f8082464e4c3f852655423b3b498f",
            "b21c6f60f3f049d0aa53e76d107dd222",
            "4e13361b6ce849d2964fbdabae8f54ed"
          ]
        },
        "outputId": "1c592510-959f-4e21-8383-a6fb9368375f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please record the audio you wish to drive the animation with. Remember to enable your microphone in Chrome:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "655f8082464e4c3f852655423b3b498f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Button(description='Record Your Voice', style=ButtonStyle())"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown #**Choose Inputs**\n",
        "# @markdown Please select one of the available inputs. Leave the text field empty if you want to animate the character with audio or video.\n",
        "\n",
        "\n",
        "#Welcome. Today we will learn about the Theory of Relativity. I first came up with this method when...\n",
        "text = \"\" #@param {type:\"string\"}\n",
        "#@markdown --\n",
        "audio = True #@param {type:\"boolean\"}\n",
        "#@markdown * Either record audio from microphone or upload audio from file (.mp3 or .wav) \n",
        "record_or_upload = \"Record\" #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "record_seconds =  5#@param {type:\"number\", min:1, max:10, step:1}\n",
        "#@markdown --\n",
        "video = False #@param {type:\"boolean\"}\n",
        "\n",
        "if text != \"\" and audio or text !=\"\" and video or audio and video:\n",
        "  raise IpyExit\n",
        "\n",
        "\n",
        "if video:\n",
        "  print(\"Please upload the video you wish to drive the animation with:\\n\")\n",
        "  video_driver = \"/content/\"+getLocalFiles()\n",
        "\n",
        "  #to do: make sure only supported video formats can be uploaded\n",
        "\n",
        "elif audio:\n",
        "\n",
        "  SAMPLE_RATE = 22050\n",
        "  embedding = None\n",
        "\n",
        "  if record_or_upload == \"Record\":\n",
        "    print(\"Please record the audio you wish to drive the animation with. Remember to enable your microphone in Chrome:\\n\")\n",
        "    button = widgets.Button(description=\"Record Your Voice\")\n",
        "    button.on_click(_record_audio) \n",
        "    display(button)\n",
        "    audio_driver = \"/content/driving_audio.wav\"\n",
        "  else:\n",
        "    print(\"Please upload the audio you wish to drive the animation with:\\n\")\n",
        "    audio_driver = \"/content/\"+getLocalFiles()\n",
        "  video_driver = \"/content/driving_video.mp4\"\n",
        "\n",
        "elif text:\n",
        "  print(\"Text is currently unsupported but will be soon.. Please use either audio or video inputs for now.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  #video_driver = \"/content/driving_video.mp4\"\n",
        "  video_driver = \"/content/sing.mp4\" ###################"
      ],
      "metadata": {
        "id": "lrdqEq7wSvXL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.core import memmap\n",
        "import shutil\n",
        "\n",
        "\n",
        "#markdown #**Generate Character**\n",
        "#markdown This is likely to take a while depending on the length of your driving video. First we generate the movements of the character using the first-order-model approach, and then, if audio or text was given as input, we either synthesize audio from or use the audio provided to make the character lipsymc it using Wav2Lip.\n",
        "%cd /content/\n",
        "print(\"Animating Character with Driving Video: This might take a few minutes..\")\n",
        "animate_video(character_img, video_driver) # variables are only for showing HTML video\n",
        "final_video_driver = \"/content/vidvid.mp4\""
      ],
      "metadata": {
        "id": "cX0mfExhkPzg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3eb673-0428-4431-8efe-16926741b549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Animating Character with Driving Video: This might take a few minutes..\n",
            "/content/first-order-model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Warning: the frame size for reading (320, 568) is different from the source frame size (568, 320).\n",
            " 60%|██████    | 177/295 [00:35<00:23,  5.00it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def copy_vidvid_mp4():\n",
        "  oname = os.path.basename(character_img).replace(\".jpg\", \"_jpg\").replace(\".png\",\"_png\")\n",
        "  srcname = \"/content/vidvid.mp4\"\n",
        "  dstname = \"/content/rst_{}_vidvid.mp4\".format(oname)\n",
        "  shutil.copy(srcname, dstname)\n",
        "  print(\"new mp4 generated\", dstname)\n",
        "\n",
        "copy_vidvid_mp4()"
      ],
      "metadata": {
        "id": "v92aAExTth6H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ea4369-cd47-4dd1-a555-9c6de8b2029d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new mp4 generated /content/rst_dhgf_jpg_vidvid.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Orsq_D2RLvo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "59ae3ad8-251d-475f-8448-210414c760dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Lipsyncing Character with Audio\n",
            "/content/Wav2Lip\n",
            "Using cuda for inference.\n",
            "Reading video frames...\n",
            "Number of frames available for inference: 555\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\", line 149, in load\n",
            "    with sf.SoundFile(path) as sf_desc:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/soundfile.py\", line 629, in __init__\n",
            "    self._file = self._open(file, mode_int, closefd)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/soundfile.py\", line 1184, in _open\n",
            "    \"Error opening {0!r}: \".format(self.name))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/soundfile.py\", line 1357, in _error_check\n",
            "    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))\n",
            "RuntimeError: Error opening '/content/driving_audio.wav': System error.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"inference.py\", line 280, in <module>\n",
            "    main()\n",
            "  File \"inference.py\", line 224, in main\n",
            "    wav = audio.load_wav(args.audio, 16000)\n",
            "  File \"/content/Wav2Lip/audio.py\", line 10, in load_wav\n",
            "    return librosa.core.load(path, sr=sr)[0]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\", line 166, in load\n",
            "    y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py\", line 190, in __audioread_load\n",
            "    with audioread.audio_open(path) as input_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/audioread/__init__.py\", line 111, in audio_open\n",
            "    return BackendClass(path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/audioread/rawread.py\", line 62, in __init__\n",
            "    self._fh = open(filename, 'rb')\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/driving_audio.wav'\n",
            "/content\n",
            "Imageio: 'ffmpeg-linux64-v3.3.1' was not found on your computer; downloading it now.\n",
            "Try 1. Download from https://github.com/imageio/imageio-binaries/raw/master/ffmpeg/ffmpeg-linux64-v3.3.1 (43.8 MB)\n",
            "Downloading: 8192/45929032 bytes (0.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2277376/45929032 bytes (5.0%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b5554176/45929032 bytes (12.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b8888320/45929032 bytes (19.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b12304384/45929032 bytes (26.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b15835136/45929032 bytes (34.5%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b18898944/45929032 bytes (41.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b22331392/45929032 bytes (48.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b25632768/45929032 bytes (55.8%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b28909568/45929032 bytes (62.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b32325632/45929032 bytes (70.4%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b35758080/45929032 bytes (77.9%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b39084032/45929032 bytes (85.1%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b41869312/45929032 bytes (91.2%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45268992/45929032 bytes (98.6%)\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b45929032/45929032 bytes (100.0%)\n",
            "  Done\n",
            "File saved as /root/.imageio/ffmpeg/ffmpeg-linux64-v3.3.1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9ddfa5fba05f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Traceability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtracability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_video_driver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mfinal_video_driver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"marked.mp4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ffmpeg -i $final_video_driver -i $audio_driver final_generated.mp4 -y &> /dev/null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c0b6c0339d92>\u001b[0m in \u001b[0;36mtracability\u001b[0;34m(video_filename)\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mmoviepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meditor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m   \u001b[0mvideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoFileClip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   machine = (mp.ImageClip('/content/noise_2.png')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/video/io/VideoFileClip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, has_mask, audio, audio_buffersize, target_resolution, resize_algorithm, audio_fps, audio_nbytes, verbose, fps_source)\u001b[0m\n\u001b[1;32m     89\u001b[0m                                          \u001b[0mtarget_resolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_resolution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                                          \u001b[0mresize_algo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_algorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                                          fps_source=fps_source)\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# Make some of the reader's attributes accessible from the clip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, print_infos, bufsize, pix_fmt, check_duration, target_resolution, resize_algo, fps_source)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         infos = ffmpeg_parse_infos(filename, print_infos, check_duration,\n\u001b[0;32m---> 33\u001b[0;31m                                    fps_source)\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_fps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'video_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moviepy/video/io/ffmpeg_reader.py\u001b[0m in \u001b[0;36mffmpeg_parse_infos\u001b[0;34m(filename, print_infos, check_duration, fps_source)\u001b[0m\n\u001b[1;32m    270\u001b[0m         raise IOError((\"MoviePy error: the file %s could not be found!\\n\"\n\u001b[1;32m    271\u001b[0m                       \u001b[0;34m\"Please check that you entered the correct \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m                       \"path.\")%filename)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: MoviePy error: the file /content/Wav2Lip/results/result_voice.mp4 could not be found!\nPlease check that you entered the correct path."
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "\n",
        "if text != \"\":\n",
        "  print(\"Generating speech from text\")\n",
        "  # generate audio\n",
        "  #audio_driver = _GENERATED AUDIO.wav_\n",
        "  audio = True\n",
        "\n",
        "if audio:\n",
        "  print(\"Lipsyncing Character with Audio\")\n",
        "  # Using Wav2Lip\n",
        "  %cd /content/Wav2Lip\n",
        "  !python inference.py --checkpoint_path \"/content/Wav2Lip/checkpoints/wav2lip_gan.pth\" --face $final_video_driver --audio $audio_driver \n",
        "  %cd /content\n",
        "  final_video_driver = \"/content/Wav2Lip/results/result_voice.mp4\"\n",
        "else:\n",
        "  audio_driver = \"/content/driver.wav\"\n",
        "  !ffmpeg -i $video_driver -q:a 0 -map 0:a \"/content/driver.wav\" -y &> /dev/null\n",
        "  !ffmpeg -i $final_video_driver -i $audio_driver -c:v copy -c:a aac merged.mp4 -y &> /dev/null\n",
        "  final_video_driver = \"merged.mp4\"\n",
        "\n",
        "# Traceability\n",
        "tracability(final_video_driver)\n",
        "final_video_driver = \"marked.mp4\"\n",
        "!ffmpeg -i $final_video_driver -i $audio_driver final_generated.mp4 -y &> /dev/null\n",
        "!ffmpeg -i $final_video_driver ai_generated_character.mp4 -y &> /dev/null\n",
        "final_video_driver = \"ai_generated_character.mp4\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# display result\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open(\"/content/final_generated.mp4\",'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "id": "-qpiaavBYhhd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "c49fc950-ee68-4bfe-ca40-833abc8b0ece"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ee95b13c1263>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbase64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mb64encode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmp4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/final_generated.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdata_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"data:video/mp4;base64,\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb64encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmp4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m HTML(\"\"\"\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/final_generated.mp4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "def copy_final_mp4():\n",
        "  oname = os.path.basename(character_img).replace(\".jpg\", \"_jpg\").replace(\".png\",\"_png\")\n",
        "  srcname = \"/content/final_generated.mp4\"\n",
        "  dstname = \"/content/rst_{}final_generated.mp4\".format(oname)\n",
        "  shutil.copy(srcname, dstname)\n",
        "  print(\"new mp4 generated\", dstname)\n",
        "\n",
        "copy_final_mp4()"
      ],
      "metadata": {
        "id": "OS0iWy4_YlsH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "0c479a22-8e56-4a11-c027-ed971d858dea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-b09b44221501>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new mp4 generated\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mcopy_final_mp4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-b09b44221501>\u001b[0m in \u001b[0;36mcopy_final_mp4\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0msrcname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/final_generated.mp4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mdstname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/rst_{}final_generated.mp4\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrcname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new mp4 generated\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdstname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mdst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mcopyfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mcopymode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_symlinks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/shutil.py\u001b[0m in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfsrc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfsrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfdst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/final_generated.mp4'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#from google.colab import files\n",
        "#files.download(final_video_driver)"
      ],
      "metadata": {
        "id": "vSRqXk10zCDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Hotgame AI_Generated_Characters (single photo to video).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "655f8082464e4c3f852655423b3b498f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_b21c6f60f3f049d0aa53e76d107dd222",
            "_dom_classes": [],
            "description": "Record Your Voice",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_4e13361b6ce849d2964fbdabae8f54ed",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "b21c6f60f3f049d0aa53e76d107dd222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e13361b6ce849d2964fbdabae8f54ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}